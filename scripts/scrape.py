# -*- coding: utf-8 -*-
"""nlp_main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1osa0Jyp6Gyb-H4XdpAVsOpox6kubXUuV
"""

!pip install bs4

from bs4 import BeautifulSoup
from requests import Session
from time import sleep
from json import dumps
from os import mkdir

try: mkdir("data")
except: pass

session = Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
})

tags = []
categories = []
queue = []
done = []

def get_soup(url):
    print(f"[+] Fetching {url}")
    res = session.get(url)

    soup = BeautifulSoup(res.content, "html.parser")

    print(f"[+] Fetched {soup.title.text} with {res.status_code} status code")

    if res.status_code == 409:
        c_key, c_value = soup.find("script").text.split(";")[0].split(" = ")[1].replace('"', "").split("=")
        print("[+] Setting cookie...")
        session.cookies.set(c_key, c_value)
        return get_soup(url)
    return soup

def run_post(url, first=False):
    soup = get_soup(url)

    if first:
        for cat in soup.find_all("li", {"class": "cat-item"}):
            link = cat.find("a")["href"]

            categories.append(link)

    paragraphs = []

    for p in soup.find_all("p"):
        if not p.has_attr("class"):
            paragraphs.append(p.text)

    local_tags = []
    local_queue = []

    for a in soup.find_all("a", {"class": "tag-cloud-link"}):
        if not a["href"] in tags: local_tags.append(a["href"])

    tags.extend(local_tags)

    for similar_post in soup.find_all("h3", {"class": "jeg_post_title"}):
        link = similar_post.find("a")["href"]
        if not link in queue and not link in done: local_queue.append(link)

    queue.extend(local_queue)

    print(f"[+] Found {len(local_tags)} tags, {len(local_queue)} similar posts...")

    done.append(url)

    if url in queue: queue.remove(url)

    with open("db.json", "w") as db:

        data = {
            "categories": categories,
            "tags": tags,
            "queue": queue,
            "done": done,
        }

        db.write(dumps(data, indent=4))

    with open(f"data/{soup.title.text}.txt", "w") as file:
        file.write("".join(paragraphs))

seed_url = "https://akannews.com/azuma-nelson-ho-ns%c9%9bm/"

run_post(seed_url)

print(f"[+] Finding posts in tags...")

for url in tags:
    posts = get_soup(url).find_all("a", {"class": "elementor-post__thumbnail__link"})

    for post in posts:
        link = post["href"]
        if not link in queue and not link in done: queue.append(link)

    print("[+] Sleeping for 10s")
    sleep(10)

for url in queue:
    run_post(url)
    print("[+] Sleeping for 15s...")
    sleep(15)